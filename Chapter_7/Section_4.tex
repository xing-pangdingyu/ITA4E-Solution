\documentclass[12pt,reqno]{amsart}
%\documentclass[../Solutions_Introduction_to_Algorithms.tex]{subfiles}
\usepackage{amsmath,amsfonts,amscd,amssymb,epsf,color,enumerate,graphicx,url}
\usepackage{algorithm, algorithmic}
\usepackage{forest, tikz, xcolor}
\usetikzlibrary{matrix, positioning}
\setlength{\oddsidemargin}{-0.2in}%
\setlength{\evensidemargin}{-0.2in}%
\setlength{\textwidth}{6.6in}%
\setlength{\topmargin}{-0.5in}%
 \setlength{\textheight}{9.5in}%
 \definecolor{orange}{rgb}{1,0.5,0}
 \pagestyle{plain}
\linespread{1.3}
\usepackage[small]{caption}
\newcommand{\pa}{\partial}
\newcommand{\va}{\vspace{0.4cm}}
\newcommand{\di}{\displaystyle}
\newcommand{\disp}{\displaystyle}


% turn on \answertrue to show the solution
% turn on \answerfalse to hide the solution
\newif\ifanswer
\answertrue
%\answerfalse



\begin{document}
\noindent {\footnotesize Introduction to Algorithms}\hspace{10.5cm} {\footnotesize Solutions}

\vspace{0.5cm}
\hspace{5.5cm}\textbf{\large Exercises in Section 7.4}
\vspace{0.5cm}

\begin{enumerate}[1.]

\item Show that the recurrence $T(n) = \max{\{T(q) + T(n - q - 1): 0\leq q\leq n - 1\}} + \Theta(n)$ has a lower bound of $T(n) = \Omega(n^2)$.

\ifanswer
\noindent {\bf Solution}
Assume $T(k) \geq ck^2$ for $k < n$. By substitution, we have
\begin{align*}
T(n) &= \max{\{cq^2 + c(n - q - 1)^2: 0\leq q\leq n - 1\}} + \Theta(n)\\
&= c\cdot\max{\{q^2 + (n - q - 1)^2: 0\leq q\leq n - 1\}} + \Theta(n),
\end{align*}
where the maximum is given by
\begin{align*}
q^2 + (n - q - 1)^2 &= q^2 + (n - 1)^2 - 2q(n - 1) + q^2\\
&= (n - 1)^2 + 2q(q - (n - 1))\\
&\leq (n - 1)^2.
\end{align*}
We have to check this upper bound is tight. Take $q = n - 1$, it is easy to see the term after $(n - 1)^2$ vanishes. Therefore, the maximum is indeed $(n - 1)^2$. Substitute to the recurrence, we have
\begin{align*}
T(n) &= c(n - 1)^2 + \Theta(n)\\
&= cn^2 + (\Theta(n) - 2cn) + 1\\
&\geq cn^2.
\end{align*}
for sufficiently small $c$. Hence, $T(n) = \Omega(n^2)$. \qed
\vspace{1cm}



\item Show that quicksort's best-case running time is $\Omega(n\lg{n})$.

\ifanswer
\noindent {\bf Solution}
The recurrence associated to the best case for quicksort is
$$
T(n) = \min{\{T(q) + T(n - 1 - q): 0\leq q\leq n - 1\}} + \Theta(n)
$$
Assume $T(k) \geq ck\lg{k}$ for $k < n$. By substitution, we have
\begin{align*}
T(n) &= \min{\{cq\lg{q} + c(n - 1 - q)\lg{(n - 1 - q)}: 0\leq q\leq n - 1\}} + \Theta(n)\\
&= c\cdot\min{\{q\lg{q} + (n - 1 - q)\lg{(n - 1 - q)}: 0\leq q\leq n - 1\}} + \Theta(n).
\end{align*}
We define the function
\begin{align*}
f(x) = x\lg(x) + (n - 1 - x)\lg{(n - 1 - x)}.
\end{align*}
The first derivative of $f(x)$ is
\begin{align*}
f'(x) &= \frac{d}{dx}(x\lg{x}) + \frac{d}{dx}\left((n - 1 - x)\lg{(n - 1 - x)}\right)\\
&= (\lg{x} + 1) - (\lg{(n - 1 - x)} + 1)\\
&= \lg{\left(\frac{x}{n - 1 - x}\right)},
\end{align*}
which evaluates to $0$ at $x = (n - 1)/2$, where we have
$$
f(\frac{n - 1}{2}) = (n - 1)\lg{\left(\frac{n - 1}{2}\right)}.
$$
Therefore, the minimum appearing in the recurrence is bounded from below by $f(\frac{n - 1}{2}) = (n - 1)\lg{\left(\frac{n - 1}{2}\right)}$. Substituting to the recurrence, we get
\begin{align*}
T(n) &\geq c(n - 1)\lg{\left(\frac{n - 1}{2}\right)} + \Theta(n)\\
&= c(n - 1)\left(\lg{(2n - 2)} - 2\right) + \Theta(n)\\
&\geq c(n - 1)\left(\lg{n} - 2\right) + \Theta(n)\\
&= cn\lg{n} - 2cn - c\lg{n} + 2c + \Theta(n)\\
&\geq cn\lg{n}
\end{align*}
by taking sufficiently small $c$ such that all other terms are dominated by the function hidden in $\Theta(n)$. Hence, we conclude $T(n) = \Omega(n\lg{n})$. \qed
\vspace{1cm}



\item Show that the expression $q^2 + (n - q - 1)^2$ achieves its maximum value over $q = 0, 1, \dots, n - 1$ when $q = 0$ or $q = n - 1$.

\ifanswer
\noindent {\bf Solution}
Note that
\begin{align*}
    q^2 + (n - q - 1)^2 &= q^2 + (n - 1)^2 - 2q(n - 1) + q^2\\
    &= (n - 1)^2 + 2q(q - (n - 1))\\
    &\leq (n - 1)^2,
\end{align*}
where the equality achieves if and only if $q = 0$ or $q = n - 1$. \qed
\vspace{1cm}



\item Show that \textsc{Randomized-Quicksort}'s expected running time is $\Omega(n\lg{n})$.

\ifanswer
\noindent {\bf Solution}
With exactly the same justification in the proof of \textit{Lemma 7.1}, the running time of \textsc{Randomized-Quicksort} is $\Omega(X)$, where $X$ is the number of elements comparisons performed. Note that we are looking for a lower bound, so we can ignore the time spent outside of the \textbf{for} loop. Then, in the proof of \textit{Theorem 7.4} we have
$$
E[X] = \sum_{i = 1}^{n - 1}\sum_{k = 1}^{n}{\frac{2}{k}}.
$$
In fact, the harmonic series is not only $O(\lg{n})$, but a tight $\Theta(\lg{n})$ (see equation (A.9) on page 1142). Therefore, we have
$$
E[X] = \sum_{i = 1}^{n - 1}{\Theta(\lg{n})} = \Theta{(n\lg{n})}.
$$
Hence, the expected running time of \textsc{Randomized-Quicksort} is $\Omega(E[X]) = \Omega{(n\lg{n})}$. \qed
\vspace{1cm}



\item Coarsening the recursion, as we did in Problem 2-1 for merge sort, is a common way to improve the running time of quicksort in practice. We modify the base case of the recursion so that if the array has fewer than $k$ elements, the subarray is sorted by insertion sort, rather than by continued recursive calls to quicksort. Argue that the randomized version of this sorting algorithm runs in $O(nk + n\lg{(n/k)})$ expected time. How should you pick $k$, both in the theory and in practice?

\ifanswer
\noindent {\bf Solution}
As the leaves of the recursion tree, there are expected to be $O(n/k)$ calls of insertion sort, with $O(k^2)$ running time for each. So, the total running time of all calls of insertion sort is $O(O(n/k)\cdot O(k^2)) = O(nk)$.\\
Again, the recursion tree have an expected depth of $O(\lg{(n/k)})$, with running time $O(n)$ on each level. Therefore, the running time for all internal nodes (that is, other than the leaves) is $O(O{\lg{(n/k)}\cdot n}) = O(n\lg{(n - k)})$. Finally, we conclude the expected running time of this sorting algorithm is $O(nk + n\lg{(n/k)})$.\\
Suppose the function hidden in the $O$-notation is $T(n, k) = \alpha nk + \beta n\lg{(n / k)}$, we have
$$
\frac{\partial}{\partial k}{T(n, k)} = \alpha n - \beta\frac{n}{k\ln{2}},
$$
which equals $0$ when $k = \frac{\beta}{\alpha\ln{2}}$. So, in this case, we should pick $k$ = $\lfloor\frac{\beta}{\alpha\ln{2}}\rfloor$ or $\lceil\frac{\beta}{\alpha\ln{2}}\rceil$.\\
In practice, the case could be more complicated. For example, the function hidden in the $O$-notation possibly has extra asymptotically smaller terms, and the restriction on the size of $n$ can also affect the choice of $k$. One should consider multiple factors before making the appropriate choice of $k$.
\vspace{1cm}



\item Consider modifying the $\textsc{Partition}$ procedure by randomly picking three elements from subarray $A[p: r]$ and partitioning about their median (the middle value of the three elements). Approximate the probability of getting worst than an $\alpha$-to-$(1 - \alpha)$ split, as a function of $\alpha$ in the range $0 < \alpha < 1/2$.

\ifanswer
\noindent {\bf Solution}
%Since we are randomly picking elements from the subarray $A$, without loss of generality, we may assume $A$ is sorted. Let $n = r - p + 1$ be the size of the subarray, and suppose $n\geq 3$. There are $\binom{n}{3}$ ways with equal probability to choose $3$ elements from $n$ elements, and each combination determines a unique median. In Exercise 7.2.6, we have shown that the index returned by $\textsc{Partition}$ is exactly the index of the pivot in the sorted subarray. For each $p\leq i \leq r$, $A[i]$ is the median of a combination if and only if the three chosen elements are one less than $A[i]$, one equals $A[i]$, and one greater than $A[i]$. There are $(i - p)(1)(r - i)$ such combinations. So, the probability of $A[i]$ being the pivot, resulting a split of $A$ at $i$, is
%$$
%P(A\textit{ splits at }i) = \frac{(i - p)(r - i)}{\binom{n}{3}}.
%$$
%Here is a trick to transform the numerator:
%\begin{align*}
%    (i - p)(r - i) &= \left(i - \frac{p}{2} - \frac{p}{2}\right)\left(\frac{r}{2} + \frac{r}{2} - i\right)\\
%    &= \left(i - \frac{p}{2} - \left(\frac{r}{2} - \frac{n}{2} + \frac{1}{2}\right)\right)\left(\left(\frac{n}{2} + \frac{p}{2} - \frac{1}{2}\right) + \frac{r}{2} - i\right)\\
%    &= \left(\frac{n - 1}{2} + \left(i - \frac{p + r}{2}\right)\right)\left(\frac{n - 1}{2} - \left(i - \frac{p + r}{2}\right)\right)\\
%    &= \left(\frac{n - 1}{2}\right)^2 - \left(i - \frac{p + r}{2}\right)^2.
%\end{align*}
%Given $0 < \alpha < 1/2$, we compute the probability of $\lceil p + \alpha n - 1\rceil \leq i \leq \lfloor r - \alpha n + 1\rfloor$:
%$$
%P(\textit{better than }\alpha\textit{-to-}(1 - \alpha)) = \sum_{i = \lceil p + \alpha n - 1\rceil }^{\lfloor r - \alpha n + 1\rfloor}{\frac{\left(\frac{n - 1}{2}\right)^2 - \left(i - \frac{p + r}{2}\right)^2}{\binom{n}{3}}}.
%$$
%For simplicity, we further assume $p + r$ is even. The odd case is similar but with tedious analysis because of the floor and ceiling calculations. Applying change of index gives
%\begin{align*}
%    \sum_{i = \lceil p + \alpha n - 1\rceil }^{\lfloor r - \alpha n + 1\rfloor}{\frac{\left(\frac{n - 1}{2}\right)^2 - \left(i - \frac{p + r}{2}\right)^2}{\binom{n}{3}}} &= 2\sum_{i = \frac{p + r}{2} + 1}^{\lfloor r - \alpha n + 1\rfloor}{\frac{\left(\frac{n - 1}{2}\right)^2 - \left(i - \frac{p + r}{2}\right)^2}{\binom{n}{3}}} + \frac{\left(\frac{n - 1}{2}\right)^2}{\binom{n}{3}}\\
%    &= 2\sum_{i = 1}^{\lfloor \frac{n}{2} - \alpha n + \frac{1}{2}\rfloor}{\frac{\left(\frac{n - 1}{2}\right)^2 - i^2}{\binom{n}{3}}} + \frac{\left(\frac{n - 1}{2}\right)^2}{\binom{n}{3}}\\
%    &= \frac{\left(2\lfloor \frac{n}{2} - \alpha n + \frac{1}{2}\rfloor + 1\right)\left(\frac{n - 1}{2}\right)^2 - 2\sum_{i = 1}^{\lfloor \frac{n}{2} - \alpha n + \frac{1}{2}\rfloor}{i^2}}{\binom{n}{3}}.
%\end{align*}
%With the formula of sum of squares, we have
%$$
%\sum_{i = 1}^{\lfloor \frac{n}{2} - \alpha n + \frac{1}{2}\rfloor}{i^2} = \frac{\left(\lfloor \frac{n}{2} - \alpha n + \frac{1}{2}\rfloor\right)\left(\lfloor \frac{n}{2} - \alpha n + \frac{1}{2}\rfloor + 1\right)\left(2\lfloor \frac{n}{2} - \alpha n + \frac{1}{2}\rfloor + 1\right)}{6}.\\
%$$
%Now, we start to find the approximate value of this probability. By ignoring all dominated constants, floors and ceilings, we have
%$$
%\sum_{i = 1}^{\lfloor \frac{n}{2} - \alpha n + \frac{1}{2}\rfloor}{i^2} \approx \frac{\left(\frac{n}{2} - \alpha n\right)^3}{3} = n^3\left(\frac{1}{24} - \frac{\alpha}{12} + \frac{\alpha^2}{6} - \frac{\alpha^3}{3}\right).\\
%$$
%Hence,
%\begin{align*}
%P(\textit{better than }\alpha\textit{-to-}(1 - \alpha)) &\approx \frac{\left(n - 2\alpha n\right)\left(\frac{n^2}{4}\right) - 2n^3\left(\frac{1}{24} - \frac{\alpha}{12} + \frac{\alpha^2}{6} - \frac{\alpha^3}{3}\right)}{\frac{n^3}{6}}\\
%&= \left(\frac{3}{2} - 3\alpha\right) - \left(\frac{1}{2} - \alpha + 2\alpha^2 - 4\alpha^3\right)\\
%&= 1 - 2\alpha - 2\alpha^2 + 4\alpha^3.
%\end{align*}
For simplicity, we assume the elements in $A$ are $1, 2, \dots, n$, and one element can be taken multiple times which does not affect the result. We want to find the probability of getting worse than an $\alpha$-to-$(1 - \alpha)$ split, that is, the median locates at $[1, \alpha]$ or $[1 - \alpha, 1]$. Since these two intervals are symmetric, we need to find one side only. Note that the median is in $[1, \alpha]$ if and only if at least two out of three chosen elements are in $[1, \alpha]$. The probability that exactly two elements are in $[1, \alpha]$ is
$$
\binom{3}{2}\alpha^2(1 - \alpha) = 3\alpha^2 - 3\alpha^3,
$$
and the probability that all three elements are in $[1, \alpha]$ is $\alpha^3$. Therefore, the probability that the median is in $[1, \alpha]$ is
$$
(3\alpha^2 - 3\alpha^3) + \alpha^3 = 3\alpha^2 - 2\alpha^3.
$$
Symmetric to $[1 - \alpha, 1]$, the final answer is $6\alpha^2 - 4\alpha^3$.




\end{enumerate}

\end{document}



