\documentclass[12pt,reqno]{amsart}
%\documentclass[../Solutions_Introduction_to_Algorithms.tex]{subfiles}
\usepackage{amsmath,amsfonts,amscd,amssymb,epsf,color,enumerate,graphicx,url}
\usepackage{algorithm, algorithmic}
\usepackage{forest, tikz, xcolor}
\usepackage{parskip}
\usetikzlibrary{matrix, positioning}
\usetikzlibrary{positioning,arrows.meta}
\setlength{\oddsidemargin}{-0.2in}%
\setlength{\evensidemargin}{-0.2in}%
\setlength{\textwidth}{6.6in}%
\setlength{\topmargin}{-0.5in}%
 \setlength{\textheight}{9.5in}%
 \definecolor{orange}{rgb}{1,0.5,0}
 \pagestyle{plain}
\linespread{1.3}
\usepackage[small]{caption}
\newcommand{\pa}{\partial}
\newcommand{\va}{\vspace{0.4cm}}
\newcommand{\di}{\displaystyle}
\newcommand{\disp}{\displaystyle}


% turn on \answertrue to show the solution
% turn on \answerfalse to hide the solution
\newif\ifanswer
\answertrue
%\answerfalse



\begin{document}
\noindent {\footnotesize Introduction to Algorithms}\hspace{10.5cm} {\footnotesize Solutions}

\vspace{0.5cm}
\hspace{5.5cm}\textbf{\large Exercises in Section 3.2}
\vspace{0.5cm}

\begin{enumerate}[1.]

\item Let $f(n)$ and $g(n)$ be asymptotically nonnegative functions. Using the basic definition of $\Theta$-notation, prove that $\max{\{f(n), g(n)\}} = \Theta(f(n) + g(n))$.
\vspace{0.5cm}

\ifanswer
\noindent {\bf Solution}

\textit{Proof.} Since $f(n)$ and $g(n)$ are asymptotically nonnegative, take $n_0$ such that $f(n) \geq 0$ and $g(n) \geq 0$. Take $c_1 = 1/2$ and $c_2 = 1$. It follows that $$0 \leq \frac{f(n) + g(n)}{2} \leq \max{\{f(n), g(n)\}} \leq f(n) + g(n)$$ for all $n \geq n_0$. \qed

\vspace{1cm}



\item Explain why the statement, ``The running time of algorithm $A$ is at least $O(n^2)$,'' is meaningless.

\vspace{0.5cm}

\ifanswer
\noindent {\bf Solution}

$O(n^2)$ is an upper bound. You can't say a number is at least bounded above by 42.

\vspace{1cm}



\item Is $2^{n + 1} = O(2^n)$? Is $2^{2n} = O(2^n)$?
\vspace{0.5cm}

\ifanswer
\noindent {\bf Solution}

\begin{itemize}
    \item Yes, $2^{n + 1} = 2\cdot 2^n = O(2^n)$.
    \item No, $2^{2n} = \omega(2^n)$. For any constant $c > 0$, take $n_0 = \lceil\lg c\rceil + 1$, it follows that $$0 \leq c\cdot 2^n < 2^n \cdot 2^n = 2^{2n}$$ for all $n \geq n_0$.
\end{itemize}

\vspace{1cm}



\item Prove Theorem~3.1.
\vspace{0.5cm}

\ifanswer
\noindent {\bf Solution}

Recall Theorem~3.1: For any two functions $f(n)$ and $g(n)$, we have $f(n) = \Theta(g(n))$ if and only if $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$.

\textit{Proof.} The forward direction is trivial. For the reverse direction, assume $f(n) = O(g(n))$ and $f(n) = \Omega(g(n))$. Let $c_1, n_1$ be constants such that $0 \leq f(n) \leq c_1\cdot g(n)$ for all $n \geq n_1$ and $c_2, n_2$ be constants such that $0 \leq c_2\cdot g(n) \leq f(n)$ for all $n \geq n_2$. It follows that $$0 \leq c_2 \cdot g(n) \leq f(n) \leq c_1\cdot g(n)$$ for all $n \geq \max{\{n_1, n_2\}}$, so $f(n) = \Theta(g(n))$. \qed

\vspace{1cm}



\item Prove that the running time of an algorithm is $\Theta(g(n))$ if and only if its worst-case running time is $O(g(n))$ and its best-case running time is $\Omega(g(n))$.

\vspace{0.5cm}

\ifanswer
\noindent {\bf Solution}

\textit{Proof.} The forward direction is trivial. For the reverse direction, let $T(n)$ be the running time of the algorithm, and assume $W(n) = O(g(n))$ and $B(n) = \Omega(g(n))$ be the worst-case and best case running times, respectively. Take $n_1$ such that $W(n) \geq T(n)$ for all $n\geq n_1$ and $n_2$ such that $B(n) \leq T(n)$ for all $n \geq n_2$.

Since $W(n) = O(g(n))$ and $B(n) = \Omega(g(n))$, there are constants $c_1, c_2$ and $n_3, n_4$ such that $$0 \leq W(n) \leq c_1\cdot g(n),$$ for all $n\geq n_3$, and $$0\leq c_2\cdot g(n) \leq B(n)$$ for all $n \geq n_4$. It follows that $$0 \leq c_2\cdot g(n) \leq B(n) \leq T(n) \leq W(n) \leq c_1\cdot g(n)$$ for all $n \geq \max{\{n_1, n_2, n_3, n_4\}}$, so $T(n) = \Theta(g(n))$. \qed

\vspace{1cm}



\item Prove that $o(g(n))\cap \omega(g(n))$ is the empty set.

\vspace{0.5cm}

\ifanswer
\noindent {\bf Solution}

\textit{Proof:} Suppose there are function $h(n)$ and $g(n)$ such that $h(n) = o(g(n))$ and $h(n) = \omega(g(n))$. Take $n_1, n_2$ such that $0 \leq h(n) < g(n)$ for all $n \geq n_1$ and $0 \leq g(n) < h(n)$ for all $n \geq n_2$. Let $n_0 = \max{\{n_1, n_2\}}$, we would have $h(n_0) < g(n_0) < h(n_0)$, which is a consideration. \qed

\vspace{1cm}



\item We can extend our notation to the case of two parameters $n$ and $m$ that can go to
$\infty$ independently at different rates. For a given function $g(n,m)$, we denote by
$O(g(n,m))$ the set of functions
\begin{align*}
O(g(n,m)) = \{f(n,m):\ &\text{there exist positive constants } c, n_0, m_0\\
&\text{such that } 0 \leq f(n,m) \leq cg(n,m)\\
&\text{for all } n \geq n_0 \text{ or } m \geq m_0\}.
\end{align*}
Give corresponding definitions for $\Omega(g(n,m))$ and $\Theta(g(n,m))$.


\vspace{0.5cm}

\ifanswer
\noindent {\bf Solution}

\begin{align*}
\Omega(g(n,m)) = \{f(n,m):\ &\text{there exist positive constants } c, n_0, m_0\\
&\text{such that } 0 \leq cg(n,m) \leq f(n,m)\\
&\text{for all } n \geq n_0 \text{ or } m \geq m_0\}.
\end{align*}

\begin{align*}
\Theta(g(n,m)) = \{f(n,m):\ &\text{there exist positive constants } c_1, c_2, n_0, m_0\\
&\text{such that } 0 \leq c_1g(n,m) \leq f(n,m) \leq c_2g(n,m)\\
&\text{for all } n \geq n_0 \text{ or } m \geq m_0\}.
\end{align*}

\vspace{1cm}


\end{enumerate}

\end{document}



